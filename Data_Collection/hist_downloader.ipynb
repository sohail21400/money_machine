{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import signal\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_datareader.data as web\n",
    "import yfinance as yf\n",
    "import pickle\n",
    "from yahoo_fin import stock_info\n",
    "from dateutil.parser import parse\n",
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TimeoutError(Exception):\n",
    "    def __init__(self, value = \"Timed Out\"):\n",
    "        self.value = value\n",
    "    def __str__(self):\n",
    "        return repr(self.value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# https://stackoverflow.com/questions/35490555/python-timeout-decorator\n",
    "def timeout(seconds_before_timeout):\n",
    "    def decorate(f):\n",
    "        def handler(signum, frame):\n",
    "            raise TimeoutError()\n",
    "        def new_f(*args, **kwargs):\n",
    "            old = signal.signal(signal.SIGALRM, handler)\n",
    "            old_time_left = signal.alarm(seconds_before_timeout)\n",
    "            if 0 < old_time_left < seconds_before_timeout: # never lengthen existing timer\n",
    "                signal.alarm(old_time_left)\n",
    "            start_time = time.time()\n",
    "            try:\n",
    "                result = f(*args, **kwargs)\n",
    "            finally:\n",
    "                if old_time_left > 0: # deduct f's run time from the saved timer\n",
    "                    old_time_left -= time.time() - start_time\n",
    "                signal.signal(signal.SIGALRM, old)\n",
    "                signal.alarm(old_time_left)\n",
    "            return result\n",
    "        new_f.func_name = f.func_name\n",
    "        return new_f\n",
    "    return decorate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_date(string, fuzzy=False):\n",
    "    \"\"\"\n",
    "    Return whether the string can be interpreted as a date.\n",
    "\n",
    "    :param string: str, string to check for date\n",
    "    :param fuzzy: bool, ignore unknown tokens in string if True\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parse(string, fuzzy=fuzzy)\n",
    "        return True\n",
    "\n",
    "    except ValueError:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save(df, fn):\n",
    "    df = df[['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']]\n",
    "    df.to_csv(fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run(args):\n",
    "    current_path = os.path.dirname(os.path.abspath(__file__))\n",
    "    hist_path = os.path.join(current_path, '..', 'data')\n",
    "    end_date = datetime.today()\n",
    "    #start_date = end_date + timedelta(days=-5 * 365)\n",
    "    start_date = datetime(2006, 1, 1)\n",
    "\n",
    "    if args.sym:\n",
    "        symbols = args.sym.split('+')\n",
    "        if 'grp_all' in symbols:\n",
    "            print('Downloading stock group all .............')\n",
    "            df_stocks = pd.read_csv(os.path.join(hist_path, 'all_stocks.csv'), header=None)\n",
    "            df_stocks = df_stocks.iloc[4080:]\n",
    "            for idx, r in df_stocks.iterrows():\n",
    "                s = r.iloc[0]\n",
    "                try:\n",
    "                    data = yf.download(s, start=start_date, end=end_date)\n",
    "                    save(data, os.path.join(hist_path, f'{s}.csv'))\n",
    "                    print(f'{s} is downloaded')\n",
    "                    time.sleep(3)\n",
    "                except Exception as e:\n",
    "                    print(f'{s} failed. {str(e)}')\n",
    "        elif 'grp_index' in symbols:\n",
    "            print('Downloading stock group index .............')\n",
    "            s_dict = {'^GSPC': 'SPX', '^DJI': 'DJI', '^NDX': 'NDX', '^RUT': 'RUT', 'VIX': 'VIX'}\n",
    "            for k, v in s_dict.items():\n",
    "                try:\n",
    "                    data = web.DataReader(name=k, data_source='yahoo', start=start_date, end=end_date)\n",
    "                    save(data, os.path.join(hist_path, f'{v}.csv'))\n",
    "                    print(f'{v} is downloaded')\n",
    "                    time.sleep(3)\n",
    "                except Exception as e:\n",
    "                    print(f'{v} failed. {str(e)}')\n",
    "        elif 'grp_dow' in symbols:\n",
    "            print('Downloading stock group dow30 .............')\n",
    "            df_stocks = pd.read_csv(os.path.join(hist_path, 'dow30.csv'), header=None)\n",
    "            for idx, row in df_stocks.iterrows():\n",
    "                try:\n",
    "                    data = web.DataReader(name=row[0], data_source='yahoo', start=start_date, end=end_date)\n",
    "                    save(data, os.path.join(hist_path, f'{row[0]}.csv'))\n",
    "                except Exception as e:\n",
    "                    print(f'{row[0]} failed. {str(e)}')\n",
    "        elif 'grp_sector' in symbols:\n",
    "            print('Downloading stock group sector .............')\n",
    "            df = pd.read_csv(os.path.join(hist_path, 'sectoretf.csv'), header=None)\n",
    "            for idx, row in df.iterrows():\n",
    "                try:\n",
    "                    data = web.DataReader(name=row[0], data_source='yahoo', start=start_date, end=end_date)\n",
    "                    save(data, os.path.join(hist_path, f'{row[0]}.csv'))\n",
    "                except Exception as e:\n",
    "                    print(f'{row[0]} failed. {str(e)}')\n",
    "            print('Sector ETF downloaded')\n",
    "        elif 'grp_country' in symbols:\n",
    "            print('Country ETF downloading .............')\n",
    "            df = pd.read_csv(os.path.join(hist_path, 'countryetf.csv'), header=None)\n",
    "            for idx, row in df.iterrows():\n",
    "                try:\n",
    "                    data = web.DataReader(name=row[0], data_source='yahoo', start=start_date, end=end_date)\n",
    "                    save(data, os.path.join(hist_path, f'{row[0]}.csv'))\n",
    "                except Exception as e:\n",
    "                    print(f'{row[0]} failed. {str(e)}')\n",
    "            print('Country ETF downloaded')\n",
    "        elif 'grp_taa' in symbols:\n",
    "            print('Mebane Faber TAA downloading .............')\n",
    "            symbols = ['SPY', 'EFA', 'TIP', 'AGG', 'VNQ', 'GLD', 'GSG']  # sp, em, bond, real estate, gold\n",
    "            for sym in symbols:\n",
    "                try:\n",
    "                    data = web.DataReader(name=sym, data_source='yahoo', start=start_date, end=end_date)\n",
    "                    save(data, os.path.join(hist_path, f'{sym}.csv'))\n",
    "                except Exception as e:\n",
    "                    print(f'{sym} failed. {str(e)}')\n",
    "            print('Mebane Faber TAA downloaded')\n",
    "        else:\n",
    "            for sym in symbols:\n",
    "                try:\n",
    "                    data = web.DataReader(name=sym, data_source='yahoo', start=start_date, end=end_date)\n",
    "                    save(data, os.path.join(hist_path, f'{sym}.csv'))\n",
    "                except Exception as e:\n",
    "                    print(f'{sym} failed. {str(e)}')\n",
    "            print(f'{args.sym} downloaded')\n",
    "\n",
    "    if args.corp:\n",
    "        df_stocks = pd.read_csv(os.path.join(hist_path, 'all_stocks.csv'), header=None)\n",
    "        df_general_info = pd.DataFrame()\n",
    "        for idx, r in df_stocks.iterrows():\n",
    "            try:\n",
    "                s = r.iloc[0]\n",
    "                ticker = yf.Ticker(s)\n",
    "                s_interested = {k: v for k, v in ticker.info.items() if k in ['sector', 'industry', 'fullTimeEmployees', 'city', 'state', 'country', 'exchange', 'shortName', 'longName']}\n",
    "                df_s = pd.DataFrame.from_dict(s_interested, orient='index', columns=[s])\n",
    "                df_s = df_s.T\n",
    "                df_general_info = pd.concat([df_general_info, df_s], axis=0)\n",
    "                print(f'{s} corp info downloaded')\n",
    "            except Exception as e:\n",
    "                print(f'{s} corp info download failed, {str(e)}')\n",
    "            time.sleep(3)\n",
    "        df_general_info.to_csv(os.path.join(hist_path, 'corporate_info.csv'))\n",
    "\n",
    "    if args.fundamental:\n",
    "        call_dict = {'balance_sheet': stock_info.get_balance_sheet,\n",
    "                     'cash_flow': stock_info.get_cash_flow,\n",
    "                     'income_statement': stock_info.get_income_statement,\n",
    "                     'stats_valuation': stock_info.get_stats_valuation,\n",
    "                     }\n",
    "\n",
    "        print('Downloading fundamentals .............')\n",
    "        outfile = os.path.join(hist_path, 'all_stocks.pkl')\n",
    "        dict_all_stocks = dict()\n",
    "        if os.path.isfile(outfile):\n",
    "            with open(outfile, 'rb') as f:\n",
    "                dict_all_stocks = pickle.load(f)\n",
    "        df_stocks = pd.read_csv(os.path.join(hist_path, 'all_stocks.csv'), header=None)\n",
    "\n",
    "        field = args.fundamental\n",
    "        func_call = call_dict[field]\n",
    "        for idx, r in df_stocks.iterrows():\n",
    "            s = r.iloc[0]\n",
    "            if s not in dict_all_stocks.keys():\n",
    "                dict_all_stocks[s] = dict()\n",
    "\n",
    "            if field in dict_all_stocks[s].keys():\n",
    "                df_old = dict_all_stocks[s][field]\n",
    "            else:\n",
    "                df_old = pd.DataFrame()\n",
    "            if not isinstance(df_old, pd.DataFrame):\n",
    "                df_old = pd.DataFrame()\n",
    "            try:\n",
    "                df_new = func_call(s)\n",
    "                df_new.set_index(df_new.columns[0], inplace=True)\n",
    "                df_new.index.name = 'Breakdown'\n",
    "                cols = [c for c in df_new.columns if is_date(c)]\n",
    "                df_new = df_new[cols]\n",
    "                # combine_first is convenient\n",
    "                df_new = df_new.combine_first(df_old)\n",
    "                dict_all_stocks[s][field] = df_new\n",
    "                print(f'{s} {field} is downloaded, having {df_new.shape} records')\n",
    "                time.sleep(3)\n",
    "            except Exception as e:\n",
    "                print(f'{s} {field} failed; {str(e)}')\n",
    "\n",
    "        with open(outfile, 'wb') as f:\n",
    "            pickle.dump(dict_all_stocks, f, pickle.HIGHEST_PROTOCOL)\n",
    "        print(f'Fundamentals {field} downloaded')\n",
    "\n",
    "    # This is adapted from https://towardsdatascience.com/sentiment-analysis-of-stocks-from-financial-news-using-python-82ebdcefb638\n",
    "    if args.sentiment:\n",
    "        print('sentiment downloading .............')\n",
    "        finwiz_url = 'https://finviz.com/quote.ashx?t='\n",
    "\n",
    "        outfile = os.path.join(hist_path, 'all_stocks.pkl')\n",
    "        dict_all_stocks = dict()\n",
    "        if os.path.isfile(outfile):\n",
    "            with open(outfile, 'rb') as f:\n",
    "                dict_all_stocks = pickle.load(f)\n",
    "        df_stocks = pd.read_csv(os.path.join(hist_path, 'intraday_stocks.csv'), header=None)\n",
    "        field = 'sentiment'\n",
    "        for idx, r in df_stocks.iterrows():\n",
    "            s = r.iloc[0]\n",
    "            if s not in dict_all_stocks.keys():\n",
    "                dict_all_stocks[s] = dict()\n",
    "\n",
    "            if field in dict_all_stocks[s].keys():\n",
    "                list_old = dict_all_stocks[s][field]\n",
    "            else:\n",
    "                list_old = []\n",
    "            if not isinstance(list_old, list):\n",
    "                list_old = []\n",
    "            try:\n",
    "                url = finwiz_url + s\n",
    "                req = Request(url=url, headers={'user-agent': 'my-app/0.0.1'})\n",
    "                response = urlopen(req)\n",
    "                # Read the contents of the file into 'html'\n",
    "                html = BeautifulSoup(response)\n",
    "                # Find 'news-table' in the Soup and load it into 'news_table'\n",
    "                news_table = html.find(id='news-table')\n",
    "                parsed_news = []\n",
    "\n",
    "                # Iterate through all tr tags in 'news_table'\n",
    "                insert_idx = 0\n",
    "                for x in news_table.findAll('tr'):\n",
    "                    # read the text from each tr tag into text\n",
    "                    # get text from a only\n",
    "                    text = x.a.get_text()\n",
    "                    # split text in the td tag into a list\n",
    "                    date_scrape = x.td.text.split()\n",
    "                    # if the length of 'date_scrape' is 1, load 'time' as the only element\n",
    "                    if len(date_scrape) == 1:\n",
    "                        tm = date_scrape[0]\n",
    "                    # else load 'date' as the 1st element and 'time' as the second\n",
    "                    else:\n",
    "                        dt = date_scrape[0]\n",
    "                        tm = date_scrape[1]\n",
    "\n",
    "                    if [s, dt, tm, text] not in list_old:\n",
    "                        print(f'insert {s} {dt} {tm} at {insert_idx}')\n",
    "                        list_old.insert(insert_idx, [s, dt, tm, text])\n",
    "                        insert_idx += 1\n",
    "                    else:\n",
    "                        print(f'skip {s} {dt} {tm}')\n",
    "\n",
    "                dict_all_stocks[s][field] = list_old\n",
    "                print(f'{s} {field} is downloaded')\n",
    "                time.sleep(3)\n",
    "            except Exception as e:\n",
    "                print(f'{s} {field} failed; {str(e)}')\n",
    "\n",
    "        with open(outfile, 'wb') as f:\n",
    "            pickle.dump(dict_all_stocks, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        print('sentiment downloaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--sym SYM] [--corp]\n",
      "                             [--fundamental FUNDAMENTAL] [--sentiment]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --ip=127.0.0.1 --stdin=9018 --control=9016 --hb=9015 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"d61b5917-4731-4cb2-b915-8a82e1b9d102\" --shell=9017 --transport=\"tcp\" --iopub=9019\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='Historical Downloader')\n",
    "    parser.add_argument('--sym', help='AAPL+AMZN or grp_all, grp_dow, grp_sector, grp_country, grp_taa')\n",
    "    parser.add_argument('--corp', action='store_true', help='corporate info')\n",
    "    parser.add_argument('--fundamental', help='balance_sheet cash_flow income_statement stats_valuation')\n",
    "    parser.add_argument('--sentiment', action='store_true')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    run(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
